log_level: "DEBUG"
memories_dir: "/FadingMemory/images/memories"
edges_dir: "/FadingMemory/images/edges"
captures_dir: "/FadingMemory/images/captures"
#bckgrnd_file: "/FadingMemory/images/backgrounds/Homeniche-16-9.jpg"  # "AUTO" or full path to image!
bckgrnd_file: "/FadingMemory/images/backgrounds/homeniche43_2.JPG"  # "AUTO" or full path to image!
#bckgrnd_file: "/FadingMemory/images/backgrounds/emptyoffice.jpg"  # "AUTO" or full path to image!
#bckgrnd_file: "/FadingMemory/images/backgrounds/Kizyard.jpeg"  # "AUTO" or full path to image!
bckgrnd_dir: "/FadingMemory/images/backgrounds/2019-08-31"
bckgrnd_prefix: "bckgrnd_"
#background: "/FadingMemory/images/backgrounds/homewall.jpg"
mergestyle: "BlackEdges"
grayscale_background: 0
model_weights_path: vgg16.npy
testing:
#     # image_width: 480
#     # image_height: 320
     image_width: 1600
     image_height: 1200
     n_channels: 3
# use snapshot after test_snapshot intervals for testing
test_snapshot: 5000
# Apply testing_threshold after sigmoid to generate binary maps set to 0.0 for continous valued edge maps
testing_threshold: 0.0
save_dir: /FadingMemory/Backend/holy-edge/hed
# Section 4.1 Loss for layer fusion
loss_weights: 1.0
# save snapshot every save_interval iterations
save_interval: 100
# validate on held out dataset
val_interval: 10
# learning rate decay (Not used with Adam currently)
learning_rate_decay: 0.1
# Apply weighted_cross_entropy_loss to outputs from each side layer
# Setting to false only loss after last conv layer is computed
deep_supervision: True
# Targets are continous if True else binary {0, 1}
target_regression: True
# Mean pixel value to subtract from BGR image
mean_pixel_value: [103.939, 116.779, 123.68]
# RGB to BGR (VGG-16 trained with BGR blame OpenCV)
channel_swap: [2, 1, 0]

# HED - CV2 configurations
edge_detector: /FadingMemory/Backend/hed-cv/hed_model
